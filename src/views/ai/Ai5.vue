<template>
  <div><div class="grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0 !gap-3.5"><h1 class="text-2xl font-bold mt-1 text-text-100">Пересказ: Базовая терминология по AI - уровень Horizons</h1>
    <p class="whitespace-normal break-words">00:00:04 --- Всем привет-привет! Это последнее видео из серии базовой терминологии по искусственному интеллекту. Сегодня будем расширять горизонты, уровень так и называется — Horizons. Будем говорить о терминах, которые расширяют кругозор и раздвигают рамки познания мира искусственного интеллекта, но при этом вызывают много дискуссий, иногда бурления и философствований. Поэтому сегодня будет интересно, но воспринимайте это с чуть большей степенью критичности. Этот уровень терминологии не связан напрямую с предыдущими, скорее немного с первым и вторым, а с третьим и четвертым, с <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Pro</code> и <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Master</code>, сильно не связан, поэтому его можно смотреть чуть в отрыве. Но все равно рекомендую просмотреть предыдущие ролики, если этого еще не делали, либо как минимум пробежать глазами текстовый файл с базовой терминологией, в котором есть абсолютно все термины всех пяти роликов. Начнем с того, чем скоро будут пугать детей.</p>
    <p class="whitespace-normal break-words">00:01:04 --- Это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>, либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Artificial General Intelligence</code>, еще его называют универсальный ИИ, сильный ИИ, общий ИИ. Много названий, но суть остается одна и та же — это система искусственного интеллекта, которая работает на уровне лучшего представителя человечества. Такой ИИ мог бы решать широчайший спектр задач, как минимум всех тех, которые может решать человек, проявлять гибкость и, главное, самообучаться. Тут есть куча трактовок от разных ученых, но в целом все они сводятся примерно к одному и тому же — такие системы могут мыслить, понимать и действовать, как это делает человек. Те из вас, кто присутствует на наших клубных созвонах, уже кучу раз сталкивались с разными формулировками <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>. Можно сказать, что сейчас практически все топовые компании, работающие с искусственным интеллектом, ставят себе целью достигнуть этого <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>, потому что это напрямую поможет им модифицировать, модернизировать производство и промышленность и вообще весь наш мир. Ровно тогда, когда будет изобретен <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>, мы получим систему, которая сможет заменять людей и даже заменять целые бизнесы, то есть делать производство дешевле. Понятно, там куча других проблем с экономикой, с целеполаганием человека возникает, но в целом <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code> хотят достичь, давайте будем честны, для того чтобы еще больше снизить стоимость производства, чтобы получить просто дешевого робота, который может делать все то, что мог делать человек. Получится или нет, пока непонятно, но мы все ближе к этому.</p>
    <p class="whitespace-normal break-words">00:02:35 --- С алгоритмической точки зрения мы уже достаточно близки. С точки зрения роботизирования за последние полтора года тоже очень большие шаги вперед были сделаны, хоть и не такие крупные, потому что всё-таки внедрение механизмов, внедрение роботов — это сильно более сложный процесс, чем внедрение алгоритмов. Алгоритм один раз изобрел, а дальше копируешь, передаешь мгновенно. Роботы надо собрать, протестировать, потом запустить производство и потом еще и транспортировать куда-то, поэтому это сильно более долгий процесс. Но в совокупности все стремятся это объединить для достижения так называемого <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>.</p>
    <p class="whitespace-normal break-words">00:03:09 --- Сразу за <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code> у нас идет то, чем уже пугают взрослых людей — это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code>, либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Artificial Super Intelligence</code>, супер искусственный интеллект, кто-то его еще называет сверхинтеллектом. И тут определение очень простое: <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code> — это система искусственного интеллекта, которая превосходит человека по всем параметрам многократно. Это следующий шаг после <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>, система, которая самообучилась и дальше получает знания с такой скоростью, что для нас это будет просто казаться магией. Где-то на уровне <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code> начинают закрадываться конкретные страхи даже у ученых умов, потому что это та система, которая будет непостижима нашему уму. Эти системы будут оперировать знаниями, до которых нам, как людям, надо будет топать еще десятки, сотни, а может и тысячи лет. И тут появляются вопросы с тем, как это дело контролировать, как с этим делом уживаться.</p>
    <p class="whitespace-normal break-words">00:04:05 --- Никто этого не знает. Куча проблем, которые не решены и, честно сказать, решаются не то чтобы сильно пока хотят. Хотя, наверное, надо было бы, потому что многие из исследователей из AI-сообщества считают, что <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code> будет достигнут очень быстро после появления <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code>. И так как <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code> — это уже конкретная цель у многих компаний, значит, и <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code> где-то там недалеко будет, а что с ним делать, пока никто особо не понимает. Вместе с <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code> всегда в разговорах возникает термин технологическая сингулярность.</p>
    <p class="whitespace-normal break-words">00:04:37 --- Это что-то, что неизвестно, что просчитать невозможно, потому что мы просто не знаем, а что там происходит. Это супер хай-левел по знаниям, до которого мы, может, никогда и не доберемся. Соответственно, технологическая сингулярность появляется примерно тогда же, когда появляется <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code>, супер искусственный интеллект, сверх искусственный интеллект, потому что там уже эти машины, эти системы будут получать знания молниеносно и оперировать терминами и знаниями, которые для нас, может быть, будут в принципе непостижимы. Может наша просто биология будет не рассчитана для того, чтобы понимать, например, какие-нибудь четырехмерные, пятимерные пространства — они будут это делать нативно. Про технологическую сингулярность, если вы слышите какие-то разговоры, то смело списывайте их на философию. Тут никакой научной базы нет и быть не может. Это та область исследований AI, которая остается на уровне философствования. Кажется, будто бы технологическую сингулярность мы сможем хоть немножко понять, ну тогда, когда она уже наступит, но это как бы уже, наверное, будет поздно.</p>
    <p class="whitespace-normal break-words">00:05:37 --- Подытоживая, технологическая сингулярность — это момент в будущем, когда технологический прогресс станет настолько быстрым и сложным, что мы, как люди, просто перестанем его воспринимать. Будем видеть магию, либо вообще ничего не видеть. Среди всего этого хаоса вы, возможно, услышите термин <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">GWM</code>. Это не <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Java Virtual Machine</code>, это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">General World Model</code>. Такая концепция, при которой AI-системы пытаются дать общее понимание физических смыслов нашего мира.</p>
    <p class="whitespace-normal break-words">00:06:06 --- В идеале системы, которые будут обладать этим представлением <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">General World Model</code>, будут понимать физический мир, в котором находятся, целостно. Если сейчас большие языковые модели оперируют какими-то текстовыми данными на входе, некоторые из них становятся мультимодальными и оперируют уже аудио-видео данными, но все это обрабатывается в едином центре, то суть <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">General World Model</code> в том, что все физические данные, которые возможно получить любыми датчиками — не только, кстати, такими, которые есть у нас, но и, например, радиоантеннами, находящимися на разных полюсах Земли — все эти данные будут обрабатываться в едином стиле и восприниматься как нечто единое. Очень интересный концепт. Некоторые компании в этом направлении двигаются. Мы уже видели новости о том, как, например, некоторые модели могут генерировать изображение мира по одной картинке, либо как диффузионные модели настраивают на бесконечную генерацию видеопотока и там Doom воспроизводят.</p>
    <p class="whitespace-normal break-words">00:07:01 --- Это что-то похожее на <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">General World Model</code>. То есть мы к этому приближаемся, и это один из альтернативных путей достижения сильного искусственного интеллекта. Когда модели начнут воспринимать все источники данных единым образом, как единую сущность, в том числе данные из физического мира, можно будет сказать, что они по когнитивным способностям будут либо сопоставимы с нами, с людьми, либо, скорее всего, превзойдут нас, что есть шажок на пути к <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">AGI</code> и <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">ASI</code>.</p>
    <p class="whitespace-normal break-words">00:07:28 --- На сегодняшний день модели настолько большие, в них настолько много параметров, что вручную оценить то, как они отвечают, как они, скажем, думают, представляется невозможным. И поэтому появляется целое направление под названием <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">explainable AI</code>, либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">X-AI</code>, на русском — интерпретируемый AI. Это целая область в AI, которая с помощью своих специальных методов, с помощью специальных алгоритмов и даже моделей нейросетей пытается понять, как же эти модели работают под капотом. Тут у математиков часто подрывает, потому что им нужна формальная верификация алгоритма, они хотят, чтобы алгоритм был человеком формально верифицируем. Их подрывает от того, что в <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Explainable AI</code> в качестве инструментов используются другие AI. Как мы можем доверять объяснению того, как работает модель, если его предоставила либо другая модель, либо эта же модель? Но, тем не менее, это применяется, это помогает понимать, как модели работают, помогает их усовершенствовать.</p>
    <p class="whitespace-normal break-words">00:08:26 --- Кроме того, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Explainable AI</code> работает над механизмами внутри самих моделей, чтобы сами модели могли лучше объяснять, как они пришли к тому либо иному выводу. Очень интересная область в изучении AI, так что теперь вы тоже про нее знаете. Так как обучение моделей, в том числе их работа, требует много компьюта, много вычислительных мощностей, появляются разные концепции, целые области в децентрализованной работе искусственного интеллекта и в децентрализованном его обучении.</p>
    <p class="whitespace-normal break-words">00:08:54 --- Это называется <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Decentralized AI</code>, либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">DAI</code>, и тут все понятно. Получаем новые продукты, новые способы того, как модели можно учить на мощностях серверов, находящихся далеко друг от друга, когда нету какого-то единого сервера. В том числе мы узнаем, как можно запускать эти модели без жесткой централизации, когда, например, каждый кусочек модели запускается на разных серверах, разбросанных по миру. Такие решения уже есть. По-моему, в инфраструктуре Apple можно разные устройства объединять в одну сеть, чтобы запускать на них, чтобы инферить модели. Это одна из концепций децентрализованного AI. Ну и, собственно, эта сфера все сильнее и сильнее развивается, поскольку обучение крупных моделей сейчас стоит просто громадных сумасшедших денег, а процессоры у нас есть. Нужно просто их объединить в сеть, и дальше учить все вместе, и запускать тоже все вместе.</p></div></div>
  <div><div class="grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0 !gap-3.5"><h1 class="text-2xl font-bold mt-1 text-text-100">Базовая терминология по AI: уровень "Horizons" (Продолжение)</h1>
    <p class="whitespace-normal break-words"><strong>00:09:48</strong> --- Теперь давайте поговорим о проблемах в области искусственного интеллекта. Есть такая область под названием Constitutional AI, и название само говорит за себя. Это область, которая изучает, как AI может быть разработан в соответствии с определёнными конституциями, политиками, правилами и рамками, которые мы в него закладываем. Эти принципы определяют желаемое качество ответа модели. Мы хотим, чтобы ответ был безопасным, этичным и правдивым. Мы хотим, чтобы он был вежливым и уважительным. Все эти рамки нужно каким-то образом встроить в LLM — может быть, запечь их в веса модели, либо применить какие-то настройки. Этим занимается область под названием Constitutional AI.</p>
    <p class="whitespace-normal break-words"><strong>00:10:11</strong> --- Есть область, которая очень сильно смежна с Constitutional AI, где-то они даже перекрывают друг друга — это AI Safety. Это область исследований, ориентированная на предотвращение вредных последствий от использования систем искусственного интеллекта. Мы не хотим, чтобы эти системы наносили вред, например, человеку. Уменьшение рисков аварий, уменьшение непреднамеренного или намеренного ущерба человечеству — вот чем занимается AI Safety. И надо сказать, что пока что, на момент конца 2024 года, складывается ощущение, что AI Safety — это больше маркетинговый термин, придуманный для того, чтобы успокоить людей, которые волнуются, что AI заберёт их работу или, может быть, уничтожит человечество. Пока что работы ведутся слабо. У Anthropic что-то получается — они пытаются делать неплохие исследования в этой области. Но остальные компании, кажется, будто бы халтурят и делают это просто для галочки, чтобы создать видимость бурной деятельности. Пока серьёзных прецедентов с вредными последствиями не было, и кажется, что пока они не появятся, эта сфера так и останется просто громким словосочетанием. Но будем надеяться, что рано или поздно она разовьётся во что-то серьёзное.</p>
    <p class="whitespace-normal break-words"><strong>00:11:10</strong> --- Очень связанный с AI Safety процесс — это уже именно процесс — называется AI Alignment, либо выравнивание, либо просто alignment. Собственно, это процесс, который обеспечивает, чтобы цели, ценности и поведение AI-моделей соответствовали ценностям тех лиц, которые их разрабатывают или заказывают. И тут не надо путать — это не про соответствие моделей ценностям человечества, потому что у нас, к сожалению, нет общих ценностей. Это именно про соответствие тем ценностям, которые в неё закладывают заказчики моделей.</p>
    <p class="whitespace-normal break-words"><strong>00:12:05</strong> --- Alignment может быть как хорошим, так и плохим — его можно по-разному интерпретировать. С одной стороны, мы, конечно же, хотели бы, чтобы модели можно было настраивать так, чтобы они были для нас полностью, ну, не безопасными, но были настроены на те же цели, что и у нас. Тогда модель не будет выдавать всяких прикольчиков, которые нам не нужны. С другой стороны, при изобретении такой технологии этой технологией обязательно воспользуются люди, у которых ценности сильно отличаются от общепринятых и, может быть, даже являются нечеловеческими — в смысле, людоедскими. Например, если какой-нибудь диктатор получит такую технологию, он непременно её использует для того, чтобы истребить неугодных ему людей. Поэтому есть ряд учёных, которые говорят, что вообще эту сферу трогать не стоит. Тут простая математика. Если мы создаём AGI без alignment, то тут вероятность, грубо говоря, того, что он захочет сделать нам что-то плохое — 50 на 50. Либо захочет, либо нет, мы не знаем. С другой стороны, если мы изобретаем alignment и у нас появляется AGI с alignment, то тут вероятность того, что его используют в злых целях — единица.</p>
    <p class="whitespace-normal break-words"><strong>00:13:13</strong> --- Технологию не остановить, и обязательно какой-нибудь придурок с лабораторией под боком будет использовать AGI в злых целях. И с этой точки зрения безопаснее не разрабатывать alignment — инструмент для этого — чем разрабатывать. Но тем не менее, лаборатории работают над alignment, потому что без alignment бюрократы и политики не хотят пропускать многие технологии в продакшн. Что тоже понятно — им-то перед людьми отчитываться надо, а не перед какими-то абстрактными инцидентами в будущем.</p>
    <p class="whitespace-normal break-words"><strong>00:13:36</strong> --- И в этом поле OpenAI решили выпендриться в своё время и создали команду под названием Super Alignment Team и ввели термин Super Alignment. Если вкратце, они просто сказали: «Знаете что, весь мир думает над обычным alignment, а мы пойдём дальше — мы будем думать над тем, как настроить на общие ценности сильный искусственный интеллект, не AGI, а ASI». И, собственно, непонятно, чем у них этот отдел занимается. Его там уже расформировывали несколько раз и пересобирали. Но звучит просто как попытка немножко похайпить и не сделать ничего практичного, потому что как мы можем выровнять на наши ценности то, чего мы точно не будем понимать, если мы не можем даже представить, как это сделать с системой, которую мы ещё понимаем?</p>
    <p class="whitespace-normal break-words"><strong>00:14:06</strong> --- Для меня это непонятно. Если для вас понятно, как можно работать над следующей ступенькой, минуя предыдущую, минуя alignment AGI — вы напишите про это в комментариях. Но в целом мне кажется, что Super Alignment — это просто пока раздутая штука. Не знаем, как регулировать AGI, а уже хотим регулировать ASI. Как-то странно.</p>
    <p class="whitespace-normal break-words"><strong>00:14:41</strong> --- Ну и в конце этого видео надо поговорить про разделение людей на два типа. Да, в AI оно есть — людей тут делят на техно-думеров и техно-оптимистов. Давайте с техно-думеров начнём. Техно-думеры — либо AI-думеры, либо просто думеры, либо AI-сейфтисты, как их ещё называют, либо децелераторы. Понятно, за что эти люди выступают. Эти люди обеспокоены экзистенциальными рисками — то есть рисками супермасштабными, которые угрожают, в принципе, существованию человечества. Они беспокоятся тем, что развитие суперискусственного интеллекта и обычного — в смысле, AGI — приведёт к тому, что эти системы будут работать не ориентированно на человеческие ценности.</p>
    <p class="whitespace-normal break-words"><strong>00:15:36</strong> --- Из этого у нас может быть куча проблем, начиная от того, что они будут работать не так, как нам надо, и заканчивая сценариями из «Терминатора». И это не шутки — там люди, именитые учёные, прямо такие теории выдвигают. Яркий представитель, наверное, из этого сообщества — это Элизар Юдковски, тот самый дядька, который написал «Гарри Поттер и методы рационального мышления». Он ещё очень большой AI-активист и один из самых именитых AI-думеров.</p>
    <p class="whitespace-normal break-words"><strong>00:16:01</strong> --- Наверное, за последние годы он ещё и самый радикальный AI-думер, потому что у него было изречение, в котором он сказал года полтора назад, что датацентры нужно бомбить ракетами, чтобы AI не вырвался из коробочки и не поработил человечество. Про «поработил человечество» там не было, но про «бомбить ракетами» Элизар говорил. Но это уже суперрадикальные техно-думеры.</p>
    <p class="whitespace-normal break-words"><strong>00:16:33</strong> --- Но в целом понятно — техно-думеры опасаются, с опаской на это смотрят. А есть техно-оптимисты. Их называют тоже по-разному — это могут быть акселераторы, либо эффектив-акселераторы, либо техно-бро. Есть ещё название E/ACC — в сокращении от Effective Accelerators. Техно-оптимисты, AI-brothers, AI-акселераторы — в общем, всё, что связано с акселерацией, с AI и с техническим оптимизмом. Тут всё понятно. Эти ребята выступают всячески за то, чтобы AGI был достигнут как можно быстрее. Там тоже есть разные причины, но основная причина в том, что AGI поднимет производительность труда на новый уровень, AGI поможет нам добывать новые знания и сильно-сильно двинет нас вперёд. И в итоге, может быть, мы даже обретём, например, бессмертие, сильно поднимем качество и длительность своей жизни.</p>
    <p class="whitespace-normal break-words"><strong>00:17:09</strong> --- Часто они за это топят. Как это бывает, у них тоже свои перекосы. Часто они полностью не обращают внимания на сопутствующие проблемы. Например, то, что удешевление производства с AI из-за быстрых сроков может повлечь неминуемую безработицу, которая не будет компенсирована новыми рабочими местами, потому что новые места не будут требовать уже людей. На это они как-то не смотрят. Поэтому это перекос в другую сторону.</p>
    <p class="whitespace-normal break-words"><strong>00:17:31</strong> --- И тут из ярких примеров, наверное, можно привести Рэя Курцвейла. Это директор по инжинирингу в Google давным-давно, один из главных футурологов, который, кстати, много чего в своей жизни правильно предсказал на фоне малых промахов. В частности, он предсказывает технологическую сингулярность ASI примерно через 20 лет и является техно-оптимистом. Вроде как не видит ничего там плохого, наоборот, считает, что мы даже где-то, может, сольёмся с этими системами и научимся понимать новые вещи, которые в физической нашей оболочке понимать бы не могли.</p>
    <p class="whitespace-normal break-words"><strong>00:18:04</strong> --- Я в последнее время ещё считаю, что Илон Маск стал техно-оптимистом, хотя он себя причисляет к техно-думерам. Он говорит, что ему тоже не нравится то, как развивается AI, как к нему пренебрежительно относятся компании, его разрабатывающие. И в своё время он говорил, что специально делает стартапы в этой сфере, чтобы тоже понимать, что происходит, и чтобы быть во главе эшелона и в нужный момент сказать «стоп» и остановить всё это дело.</p>
    <p class="whitespace-normal break-words"><strong>00:18:29</strong> --- По крайней мере, раньше он так говорил, но я давно не слышал, чтобы он так говорил. То, что он делает в XAI, пока что показывает, будто бы он либо просто приутих со своим техно-думерским мнением, потому что оно, скажем честно, менее популярно, чем техно-оптимистичное. Либо всё-таки склоняется уже больше в техно-оптимизм, особенно на фоне того, что он сейчас пробивается в топ-5, а может даже в топ-4 производителей моделей. Ну, ему выгодно придерживаться оптимистичных взглядов, чтобы инвесторы денежки давали. Но это моё личное мнение. В общем, Илон Маск — он со всех сторон очень странный персонаж.</p>
    <p class="whitespace-normal break-words"><strong>00:18:52</strong> --- В принципе, на этом всё. Я вас поздравляю — вы прошли полный курс базовой терминологии по искусственному интеллекту. После этого, я надеюсь, вам, в принципе, любая беседа — будь то беседа с обычным программистом, либо с дата-сайентистом, либо, может, даже с учёным — будет намного более понятной.</p>
    <p class="whitespace-normal break-words"><strong>00:19:22</strong> --- Я уверен, что эта серия была вам полезна. Ну, а для того чтобы возобновлять данные и знания в голове и не терять их, просто скачайте себе файлик с этой терминологией. Всё то, что я вам рассказывал в видосах, содержится в одном PDF-файле. Называется он «Базовая AI-терминология для разработчиков». Скачайте, положите где-нибудь рядом и периодически подсматривайте. И, кстати, периодически обновляйте его, потому что где-то раз в несколько месяцев я уточняю там информацию, переписываю, добавляю что-то новое.</p>
    <p class="whitespace-normal break-words"><strong>00:19:52</strong> --- Видосы, к сожалению, переписывать сложно, а файлик можно всё-таки править очень легко, поэтому в первую очередь за файликом следите. Ну и делитесь вашими впечатлениями в чате, пишите, что ещё хотите видеть в обучалках, закидывайте ваши идеи в предложку и всячески активничайте. Вы большие молодцы, я очень рад, что мы с вами вместе собрались в «Эволюции кода». На этом заканчиваем эту серию и увидимся на следующих воркшопах, созвонах и в следующих контентах.</p>
    <p class="whitespace-normal break-words"><strong>00:20:16</strong> --- С вами был Алексей Картынник, пока-пока!</p></div></div>
</template>
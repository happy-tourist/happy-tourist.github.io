<template>
  <div><div class="grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0 !gap-3.5"><h1 class="text-2xl font-bold mt-1 text-text-100">Базовая терминология по AI: уровень "Pro"</h1>
    <p class="whitespace-normal break-words">00:00:04 --- Всем привет, нейрофолкс! Это третья часть серии видео про базовую терминологию AI для разработчиков. Сегодня разбираем уровень PRO — тот уровень, который понадобится вам для того, чтобы успешно начать работать с нейросетями в разработке. То есть, использовать нейросети в разработке. В принципе, сегодняшнее видео будет одним из самых главных, потому что после просмотра его и двух предыдущих можно сказать, что вы будете владеть всей базовой терминологией, которая нужна для того, чтобы успешно развиваться в программировании с AI и коммуницировать в эволюции кода. Дальше будут еще два видео — это уровень Мастер и уровень Horizon, но они уже со звездочкой дополнительно. А сегодняшнее видео, наверное, будет самое главное из этой серии. Напоминаю, что для тех, кто не любит смотреть видео либо слушать, у нас есть замечательная текстовая версия базового AI-словаря, базы AI-терминологии для разработчиков, ссылочка на текстовую версию будет в описании. Там есть все то, про что я вам буду сегодня рассказывать, а также в будущих видео и в предыдущих. В общем, обучайтесь.</p>
    <p class="whitespace-normal break-words">00:01:01 --- Начнем с токенизации. Это процесс разбиения текста на токены, про которые мы в прошлом видео говорили, и нужно это для того, чтобы модель обучалась эффективно либо делала эффективно предсказания. Всегда проще работать не просто с буквами, не просто с символами какой-то кодировки, а модели работают с токенами. Процесс разбиения на токены называется токенизация. Соответственно, инструменты, которые делают эту токенизацию, называются токенайзеры. Токенайзеры, токенайзеры. Если бы Пятница делала песню про токенайзера, она бы называлась «Токенайзеры. Токенай-най».</p>
    <p class="whitespace-normal break-words">00:01:25 --- Дальше идет датасет — очень широкое слово в контексте генеративных моделей. Это те данные, точнее набор данных, на которых модель обучается. Датасеты могут состоять из данных в разных модальностях — это могут быть тексты, изображения, аудио, видео, 3D, все что угодно. В процессе обучения модели эти данные токенизируются и обрабатываются. Ну и конечно же данные разных типов токенизируются по-разному. Может показаться, что датасет это простая штука, ну просто там данных собрал и все, но на самом деле от качества датасета зависит то, как качественно будет работать любая модель. И датасеты — это предподготовленные данные. Подготовка данных, либо разметка данных, как это еще называют, занимает много времени и внимания человеческого, и не только человеческого. Более того, модели топовые, такие как GPT-4O, такие как Claude Sonnet, Lama 3.1 — эти модели обучались на таком здоровенном количестве данных, на таких здоровенных датасетах, что вам не снилось. То есть собрать датасет — это очень-очень-очень трудозатратная работа. Трудозатратная как по затратам человеческим для разметки этих данных, так и по затратам просто сбора данных. OpenAI для обучения своих моделей буквально пылесосил последние года с 2018 года весь интернет, чтобы набрать терабайты данных, десятки, может, и сотни, и потом их ужать до того датасета, на котором сегодня обучаются их топовые модели. То есть, собрать датасет — это далеко не тривиальная задача. Именно поэтому мы пользуемся готовыми датасетами, которые для нас уже собрали. Благо, их тысячи, а то и десятки тысяч. И благо, есть сайты, на которых это можно делать. Самый известный сайт, про который вы уже наверняка слышали — это Hugging Face. На нем есть как модели, так и тысячи, если не десятки тысяч датасетов под разные задачи. Еще есть, например, китайский аналог Hugging Face — Model School. Ну и специфичные датасеты иногда лежат на гитхабе тех людей, которые их делают. В общем-то, для разных задач есть разные датасеты. Есть датасеты как для полного обучения модели, которые занимают гигантское количество места, так и датасеты небольшие для того, чтобы файн-тюнить модели, то есть дообучать их. Про файн-тюн мы сегодня еще поговорим. Делать это быстренько, может, даже на локальных своих машинках это вы могли бы сделать. У нас, кстати, был воркшоп недавно по файн-тюну Lama 3 на задачах программирования. Можете посмотреть, там прям вживую показываем, как на живом датасете небольшом дообучить модельку, чтобы она лучше умела программировать. В общем, датасет — штука очень важная.</p>
    <p class="whitespace-normal break-words">00:04:12 --- Веса модели — это числовые параметры, которые используются для определения важности или степени влияния входного параметра на ответ модели выходной. По факту веса являются ключевыми компонентами в процессе обучения модели, поскольку именно они влияют на конечную точность и качество ответа самой модели. Если говорить по-простому, то это обычные числа, которые хранятся в файлах. Вот мы когда скачиваем веса модели, это файл. Вот в файле лежат числа. Это и есть веса. Хранятся они в разных типах данных — это могут быть <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">float32</code>, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">float16</code>. Дальше немножко про это поговорим. Если говорить про математическое определение, то веса моделей — это значения матриц, из которых состоит наша модель. И в процессе обучения, перемножая матрицы, как-то манипулируя с ними, точнее, делая какие-то манипуляции с теми матрицами, мы обучаем нашу модель. То есть под капотом при обучении делаются манипуляции с матрицами, значения которых являются весами. Если вы хотите глубже понять, как работают веса, то это уже слегка область академических математических знаний, которая, впрочем, не очень сложная, так что вы можете сами поднять их в свободное время, будет только на пользу.</p>
    <p class="whitespace-normal break-words">00:05:13 --- Есть более обобщенное понятие под названием параметры модели, которая включает в себя веса моделей, смещение весов — это еще называется offset либо bias, и гиперпараметры. Гиперпараметры — это такие параметры, которые настраиваются единожды при создании модели и потом не меняются в процессе ее обучения. В интернете понятие «вес» и «параметр» часто используют как взаимозаменяемые, что слегка является, как вы уже поняли, неверным. В целом веса с параметрами не соотносятся один к одному, потому что смещений тоже очень много. Гиперпараметрами, да, можно пренебречь, но смещений прям очень много. Но в целом нас это полностью устраивает. Когда мы видим название какой-то модели, например, Lama 3.1-70B, то вот эта вот циферка в конце 70 и значок B означает количество параметров, из которых состоит эта модель. В данном случае это означает, что Lama 3.1 данной версии содержит 70 миллиардов — поэтому B, потому что billions — параметров. Если бы там стояло 70M, было бы 70 миллионов, 70К — 70 тысяч соответственно. Поэтому в названиях модели практически всегда законфигурировано, точнее записано количество параметров, из которых состоит модель, что достаточно полезная информация, поскольку когда мы дальше будем говорить про процесс квантизации, вы поймете, что количество параметров модели напрямую коррелирует с размером файлов, в которые эти параметры записаны, соответственно и коррелирует с количеством оперативной памяти либо видеопамяти, которая вам понадобится для того, чтобы запустить эту модель, но об этом чуть дальше. Также бытует слегка ошибочное мнение о том, что чем больше параметров у модели, тем она умнее, но тут нужно понимать, что модель действительно умнее, чем больше параметров, но кроме параметров на умность влияет еще и архитектура модели, и то, как она обучалась, и датасет, на котором она обучалась. Мы уже сейчас можем видеть, как некоторые модели, например, на 8 миллиардов параметров по своей эффективности догоняют старые 70-миллиардные модели. Это все потому что, да, датасет получше, архитектуры улучшаются. В общем, не всегда больше параметров значит лучше, но часто. Однозначно это правило работает в моделях одного семейства. Например, если выпускается модель Lama 3.1 на 8 миллиардов, 70 миллиардов и 405 миллиардов параметров, то тут в рамках одного релиза, одного семейства, выпущенного вместе, однозначно модель на 405 миллиардов будет лучше модели на 70 миллиардов, и модель на 70 миллиардов будет лучше модели на 8 миллиардов параметров. Но сравнивать разные модели, разные архитектуры, разные модели того же семейства, выпущенные в разное время, по количеству параметров — это неправильно. Нужно смотреть бенчмарки. Кстати, у нас в сообществе есть замечательный список бенчмарков, по которым можно это делать. Чекайте ссылочки в описании видоса и погружайтесь в контент клуба.</p>
    <p class="whitespace-normal break-words">00:08:20 --- Накинем немного жирку и поговорим про эмбеддинги. Что же такое эмбеддинги? Эмбеддинг — это такой процесс преобразования дискретных данных, типа слова, фраза либо целое предложение, в векторное представление в многомерном пространстве модели. Да, как мы помним, модели оперируют матрицами. Матрица, если вы вдруг не помните, это набор чисел упорядоченный. Матрица второго порядка — это просто табличка сколько-то на сколько-то. И вот вектора с матрицами, ну прям очень хорошо коррелируют. Вектора могут храниться в матрицах, поскольку вектор — это набор чисел, матрица — это таблица из чисел, либо, если это двухмерная матрица, да, как в наших моделях. Соответственно, векторы могут быть выражены матрицами. Матрица, можно сказать, это такие словари для векторов в представлении моделей. И модели, собственно, этими эмбеддингами — ну вот эти вот вектора, еще не только процесс так называют, но иногда называют сами вектора эмбеддингами — модели этим всем оперируют. У них здоровенное многомерное пространство в их виртуальной башке, состоящее из этих векторов. И что нам важно понимать, для чего нужны эти эмбеддинги, потому что это очень фундаментальный процесс: чем ближе по значению слова, тем ближе они располагаются в этом многомерном пространстве. И таким образом модели находят корреляцию между словами. Вот эта вот близость в представлении, она, во-первых, математически очень удобно делается в процессе обучения модели, поскольку мы оперируем с векторами и с матрицами, а это математический объект, с которым очень понятно как работать. Есть куча всяких формул, всяких способов формальных работы с этими объектами. Ну, а во-вторых, например, если у нас есть два слова «кошка» и «собака», то они будут сильно ближе находиться в этом многомерном представлении модели, чем, например, «кошка» и «голубое». И, соответственно, когда мы спрашиваем что-то про животных домашних, то нам отвечают про котиков и собачек, а не про котиков и голубое. Как это работает подробнее, можно поднять из базового курса какого-нибудь векторного анализа, это вышмат буквально первого курса, но в целом просто понимайте, что вот эти вот вектора, они же эмбеддинги, они находятся внутри модели, и благодаря тому, что эти вектора можно совмещать друг с другом, сопоставлять, модель понимает, как связаны те либо иные слова. Но еще это процесс — процесс преобразования вот этих вот данных входных, обычных слов, предложений, фраз вот в это векторное представление. Подробнее вообще понимать, что такое вектора, что такое эмбеддинги, нужно для того, чтобы разрабатывать так называемые RAG-системы. Про эти системы мы будем говорить на уровне Мастер, поэтому пока что можете просто на подкорке отложить, что есть такие эмбеддинги, есть такой процесс. И это про сопоставление, про похожесть слов во многомерном представлении модели.</p>
    <p class="whitespace-normal break-words">00:11:09 --- Двигаем дальше. Механизм внимания — один из самых важных механизмов, который представляет из себя архитектурный подход, помогающий моделям сосредотачиваться на самых важных местах входных данных при их работе. И именно этот механизм в 2017 году положил начало трансформерам в уже, наверное, известной вам научной работе под названием «Attention is all you need». Однако я неустанно напоминаю, что механизм внимания существует благодаря научной группе из трёх человек, в состав которой входил также мой земляк Дима Богданов. И в далёком 2014 году они описали механизм внимания в рамках работы «Neural Machine Translation by Jointly Learning to Align and Translate». Так что знайте наших.</p>
    <p class="whitespace-normal break-words">00:11:39 --- Авторегрессия. Кажется, будто бы сложный термин, но на самом деле он обозначает то, что модель предсказывает следующее слово или токен на основании всех последовательностей предыдущих слов. Модель обучается генерировать последовательности, используя в качестве входных данных свои же собственные предсказания для дальнейших шагов. Ну, а сам термин авторегрессия звучит таким сложным, потому что он на самом-то деле берет начало из математической статистики. Это математическая модель временных рядов, в котором текущее значение ряда зависит от предыдущих значений этого же ряда. И основная идея авторегрессии заключается в том, что следующее значение ряда можно предсказать на основании его предыдущих значений.</p>
    <p class="whitespace-normal break-words">00:12:33 --- Температура. Не та температура, которая у нас с вами. Надеюсь, у всех у вас сейчас 36,6. Температура — это гиперпараметр, который отвечает за то, насколько креативно либо некреативно будет отвечать модель в процессе работы. Проще говоря, этот параметр влияет на распределение вероятности при выборе следующего токена. И обычно, чем больше температура, тем больше галлюцинирует, то есть, креативит модель.</p></div></div>
  <div><div class="grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0 !gap-3.5"><h1 class="text-2xl font-bold mt-1 text-text-100">Базовая терминология по AI: уровень "Pro"</h1>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">Продолжение</h2>
    <p class="whitespace-normal break-words">00:13:05 --- В целом, если температура самая минимальная выставлена в модели, а ее можно выставлять не везде, но часто можно, то минимальная <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">температура</code> обычно означает, что при подаче на вход модели одного и того же запроса вы будете получать один и тот же ответ. И тут еще развивается одна из иллюзий того, что модели работают недетерминированно. На самом деле, при температуре минимальной, нулевой температуре, модели работают вполне себе детерминированно.</p>
    <p class="whitespace-normal break-words">00:13:32 --- Однако детерминировать то количество, здоровеннейшее количество этих шагов вручную, даже куча математиков, которые существуют там на всей планете Земля, не представляется возможным за разумное количество времени, например, за время их осмысленной жизни. Поэтому можно с натяжкой сказать, что модели недетерминированы, потому что мы, как люди, не можем проверить ход мысли.</p>
    <p class="whitespace-normal break-words">00:13:55 --- Ну, изобретаются новые способы, как понять, как модель пришла к тому либо иному выводу, но эти способы часто базируются на помощи других моделей, и математики из-за этого лютуют и говорят: как это — мы не доверяем нашим моделям, но чтобы их проверять, используем другие модели. Ну, в общем, тут срач, но в целом модели детерминированы, точнее их работа и ответы детерминированы, просто очень сложно это проверить. И вот эта <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">температура</code> влияет на распределение вероятности. Выше температура — больше вероятность добавить что-то новенькое, интересненькое в ответ, ну и также больше вероятность загаллюцинировать.</p>
    <p class="whitespace-normal break-words">00:14:30 --- Поэтому температура и галлюцинации напрямую связаны. Во многих сервисах температуру регулировать вручную не дают, но при работе через API с моделями обычно температуру дают регулировать все. Так что можете поиграться на досуге. Ну вообще забавно, да? Чем больше температура, тем больше галлюцинации. Есть тут какое-то сходство с человеком. Идем дальше. <strong>Квантизация</strong> — это одна из форм сжатия моделей с потерей качества.</p>
    <p class="whitespace-normal break-words">00:14:55 --- Зачем сжимать модели? Ну понятно: для того, чтобы модели требовали меньше ресурсов при их работе, поскольку модели — достаточно прожорливые штуки. И естественно, на пути сжатия мы жертвуем чем-то, в данном случае мы жертвуем качеством ответа этих моделей. В чем заключается суть квантизации? Как я вам уже рассказывал, параметры модели хранятся как обычные числа. Как все мы с вами знаем, числа в машине хранятся в каком-то формате. Это могут быть инты, числа с плавающей запятой.</p>
    <p class="whitespace-normal break-words">00:15:25 --- И обычно в файлах, в которых хранятся параметры модели, их еще моделями иногда называют, эти числа хранятся как 32-битные числа с плавающей точкой (<code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP32</code>), либо 16-битные (<code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP16</code>), 16-разрядные, как хотите называйте. А это значит, что очень здоровенная точность у этих чисел присутствует, потому что, ну, сами понимаете, 32 бита и даже 16 бит — это очень большая точность.</p>
    <p class="whitespace-normal break-words">00:15:49 --- И эта точность не всегда нужна. Особенно на специализированных узких моделях этого много, и поэтому придумали процесс квантизации. Соответственно, его суть на базовом уровне понимания проста. Мы просто берем все параметры модели и переводим их из одного типа данных в другой, например, из <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP16</code> в какой-нибудь <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">INT8</code> (или <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code>), либо вообще <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">INT4</code> (<code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code>). Еще такая точность называется <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code> либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code>, и прямо в названиях моделей вы часто можете видеть такие приписки в конце.</p>
    <p class="whitespace-normal break-words">00:16:19 --- По меточке <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code>, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code> знаете, что это как раз таки квантизированная модель, то есть модель с уменьшенной точностью параметров. А значит, модель чуть глупее той, из которой ее сделали, но видимо не просто так квантизировали. Тут важно понимать, что квантизация — это процесс, который позволяет запускать модель на меньших ресурсах, а конкретно уменьшается размер самого файла, в котором хранятся веса.</p>
    <p class="whitespace-normal break-words">00:16:45 --- А значит, уменьшается количество оперативной либо видеопамяти, в которую эти веса загружаются, потому что при работе с моделями, для того чтобы с моделью можно было работать, инферентировать ее, мы должны загрузить все эти числа, все веса в оперативную память. К сожалению, пока что модели работают так, что все веса активны.</p>
    <p class="whitespace-normal break-words">00:17:07 --- Соответственно, если у нас модель на 13 миллиардов параметров, и каждый параметр хранится в <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP16</code>, это значит, что на каждый параметр у нас уходит примерно 2 байта информации, то мы просто умножаем 13 миллиардов параметров на 2 байта информации и получаем примерно 24 гигабайта памяти. Вот такой расчет:</p>
    <div class="relative group/copy bg-bg-000/50 border-0.5 border-border-400 rounded-lg"><div class="sticky opacity-0 group-hover/copy:opacity-100 top-2 py-2 h-12 w-0 float-right"><div class="absolute right-0 h-8 px-2 items-center inline-flex z-10"><button class="inline-flex
  items-center
  justify-center
  relative
  shrink-0
  can-focus
  select-none
  disabled:pointer-events-none
  disabled:opacity-50
  disabled:shadow-none
  disabled:drop-shadow-none border-transparent
          transition
          font-base
          duration-300
          ease-[cubic-bezier(0.165,0.85,0.45,1)] h-8 w-8 rounded-md active:scale-95 backdrop-blur-md Button_ghost__Ywhj1" type="button" aria-label="Copy to clipboard" data-state="closed"><div class="relative"><div class="flex items-center justify-center transition-all opacity-100 scale-100" style="width: 20px; height: 20px;"><svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg" class="shrink-0 transition-all opacity-100 scale-100" aria-hidden="true"><path d="M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z"></path></svg></div><div class="flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50" style="width: 20px; height: 20px;"><svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg" class="shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50" aria-hidden="true"><path d="M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z"></path></svg></div></div></button></div></div><div><pre class="code-block__code !my-0 !rounded-lg !text-sm !leading-relaxed" style="background: transparent; color: rgb(171, 178, 191); text-shadow: rgba(0, 0, 0, 0.3) 0px 1px; font-family: var(--font-mono); direction: ltr; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; line-height: 1.5; tab-size: 2; hyphens: none; padding: 1em; margin: 0.5em 0px; overflow: auto; border-radius: 0.3em;"><code style="background: transparent; color: rgb(171, 178, 191); text-shadow: rgba(0, 0, 0, 0.3) 0px 1px; font-family: var(--font-mono); direction: ltr; text-align: left; white-space: pre-wrap; word-spacing: normal; word-break: normal; line-height: 1.5; tab-size: 2; hyphens: none;"><span><span>Требуемая память = Параметры × Байты на параметр
</span></span><span>
</span><span>FP16 (16 бит = 2 байта):
</span><span>13 000 000 000 × 2 байта ≈ 24 ГБ
</span><span>
</span><span>Q8 (8 бит = 1 байт):
</span><span>13 000 000 000 × 1 байт ≈ 12 ГБ
</span><span>
</span><span>Q4 (4 бита = 0.5 байта):
</span><span>13 000 000 000 × 0.5 байта ≈ 6 ГБ
</span><span>
</span><span>Q2 (2 бита = 0.25 байта):
</span><span>13 000 000 000 × 0.25 байта ≈ 3 ГБ</span></code></pre></div></div>
    <p class="whitespace-normal break-words">Нам нужно будет около 24 гигабайт, чтобы эту модель относительно небольшую запустить.</p>
    <p class="whitespace-normal break-words">00:17:44 --- Если мы ее квантизируем, например, квантизируем ее до <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code>, то на каждый параметр у нас будет один байт информации, значит модель займет 12 гигабайт оперативной либо видеопамяти. Соответственно, если это будет <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code> (сейчас даже есть <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q2</code>), то мы будем иметь кратно меньше необходимость в памяти: 6 гигабайт памяти либо 3 гигабайта памяти. Но также не кратно, но в принципе сильно падает качество модели. И если модели <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code> можно использовать и они достаточно хороши, то <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code> и <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q2</code> они уже прям сильно просаживаются в качестве работы, проверено на практике.</p>
    <p class="whitespace-normal break-words">00:18:12 --- Знайте: если вы видите модель с приписку <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q8</code>, значит это квантизированная модель, и в принципе ее можно юзать. Но лучше, если есть ресурсы, используйте оригинал — это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP16</code> либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">FP32</code>. Квантизованная модель она всегда теряет в качестве и всегда делается для того, чтобы запускаться на менее мощном железе и работать быстрее, потому что инференс, а точнее работа с этими параметрами, с этими весами, она тоже происходит быстрее, значит и количество токенов в секунду у нас тоже возрастает, но теряем в качестве.</p>
    <p class="whitespace-normal break-words">00:18:42 --- Теперь пройдемся по обучению модели. Собственно, обучение модели, оно бывает разное. Есть базовый термин <strong>pre-trained</strong> — это самый тяжеловесный процесс, когда модель первично обучается на датасете. То есть изначально у нас с вами что есть? Есть сама модель — это программно написанный продукт, который пишется программистами и учеными в основном, на котором происходит процесс обучения.</p>
    <p class="whitespace-normal break-words">00:19:07 --- То есть мы загоняем входные параметры, получаем какие-то ответы, сравниваем их там по всяким механизмам с тем, что нам нужно, и миллион раз это переделываем до тех пор, пока модели они начинают отвечать так, как нам надо. И в итоге на выходе мы имеем вот эту, все еще вот эту вот модель, то есть написанный код, на котором все это работает, плюс веса, которые мы получили, то есть плюс матрица с цифрами, которые обоснованы вот тем обучением на том датасете, который мы проводили кучу миллионов раз, потратив кучу энергии и денег.</p>
    <p class="whitespace-normal break-words">00:19:39 --- На входе датасет и модель в виде кода, на выходе модель в виде кода и параметры-веса. И, соответственно, вот этот процесс получения из датасета весов называется <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">pre-train</code>'ом, либо обучением. Он очень долгосрочный, точнее, он очень долгий и очень дорогостоящий, поскольку, напоминаю, в датасетах для обучения крупных моделей просто не даже, наверное, не терабайты...</p>
    <p class="whitespace-normal break-words">00:20:02 --- А может, даже уже петабайты, я не знаю, текстовых данных, поскольку эти данные еще собираются годами — это просто здоровенный массив данных, и все эти данные надо кучу раз прогнать через модель. Именно поэтому топовые версии закрытых моделей обучаются месяцами, а то и дольше, и на их обучение тратятся сотни тысяч, а то и миллионы долларов. Но зато, один раз обучив, мы получаем классный продукт в виде <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Llama 3.1</code> либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">GPT-4o</code>, который дальше разными способами докручивая, можно делать еще лучше.</p>
    <p class="whitespace-normal break-words">00:20:34 --- <strong>RLHF</strong> — <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Reinforcement Learning from Human Feedback</code>, либо обучение с подкреплением на основании человеческих отзывов, человеческой реакции, человеческого фидбэка. Это такой метод обучения моделей, при котором они получают обратную связь от людей, собственно, вознаграждаются. Те самые лайки, дизлайки, которые вы видели в части GPT, используются в том числе, чтобы делать <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">reinforcement learning from human feedback</code>.</p>
    <p class="whitespace-normal break-words">00:21:02 --- То есть вместо автоматических сигналов вознаграждения используются оценки и предпочтения людей. Теперь вы знаете, что такое <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">RLHF</code>. После того как модель была обучена, ее можно файн-тюнить. <strong>Fine-tuning</strong> — это один из процессов дообучения модели, и он сильно более дешевый, чем процесс притрейна, поскольку затрагивает не все веса модели и адаптирован для того, чтобы делать модель лучше в какой-то конкретной области, а не полностью переделывает ее механизм работы.</p>
    <p class="whitespace-normal break-words">00:21:32 --- То есть не полностью переделывает ее веса. Именно через <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> из базовых обученных моделей получают модели, которые хорошо умеют в чаты, то есть умеют хорошо чатиться, либо хорошо умеют следовать инструкциям. В нашем случае это, например, программирование, когда мы конкретно говорим, что сделать модели. Также с помощью <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> иногда можно добавлять в модель новое знание, но <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> хорошо работает для улучшения специфичных вещей в модели, то есть для решения специфичных задач.</p>
    <p class="whitespace-normal break-words">00:22:00 --- Когда же мы добавляем новые знания, то эти новые знания, как бы, по идее должны по-хорошему распространяться, размазываться по всей модели. И поэтому <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> не всегда очень хорошо подходит для добавления новых знаний. Но и там есть всяческие механизмы, как делать <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> на добавление новых знаний. Но в целом <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> — это сильно более дешевый процесс дообучения модели для того, чтобы она лучше работала в конкретных задачах.</p>
    <p class="whitespace-normal break-words">00:22:25 --- И да, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> настолько дешевле предрейна, что он буквально может стоить там для open-source моделей десятки долларов. Вы можете арендовать железо либо арендовать целые сервисы, которые файн-тюнят конкретные модели, заплатить там 10, 20, максимум 100 долларов, чтобы зафайн-тюнить какую-нибудь <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Llama 3.1</code> на 70 миллиардов параметров на ваши задачи либо какие-нибудь ваши датасеты. Некоторые модели open-source можно даже файн-тюнить локально на своих компах, не тратя вообще никакие деньги.</p>
    <p class="whitespace-normal break-words">00:22:54 --- Некоторые закрытые модели также можно файн-тюнить. По-моему, OpenAI предоставляет такую возможность файн-тюнить некоторые из своих моделей через API, через их внутренние сервисы, но, естественно, закрытые модели файн-тюнить уже подороже — там уже может на тысячу долларов идти счет. Но это все еще не десятки, не сотни тысяч долларов и не миллионы. В общем, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code> — наше все. И одним из самых популярных методов <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">fine-tune</code>, наверное, является <strong>PEFT</strong> — <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Parameter Efficient Fine-tuning</code>.</p>
    <p class="whitespace-normal break-words">00:23:21 --- Суть метода проста: мы изменяем только некоторые параметры, как бы замораживая все остальное, таким образом делая модель более продуманной в какой-то конкретной области либо задаче. Ну, а если не надо перетренировать все веса, то это, соответственно, здоровенное сокращение ресурсов для этого и сокращение денежных затрат. <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">PEFT</code> — это на самом деле класс методов, и самым популярным из этого класса является метод <strong>LoRA</strong>, возможно вы про него уже слышали — <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Low Rank Adaptation</code>.</p>
    <p class="whitespace-normal break-words">00:23:50 --- Вместо изменения всех параметров модели <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">LoRA</code> добавляет специально низкоуровневые низкоранговые матрицы, таким образом специализируя модель на каких-то конкретных задачах. Чтобы не углубляться в детали, можно представить это на примере работы с программным обеспечением. Например, у нас есть здоровенная программина, и она написана нормально по всем паттернам разработки. Для того чтобы добавить в нее новый функционал, мы не переписываем всю программу — мы делаем какой-нибудь патч, дописываем какой-то модуль.</p>
    <p class="whitespace-normal break-words">00:24:20 --- Делаем это весьма абстрактно, если мы все по SOLID писали. И можно сказать, что <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">LoRA</code> примерно таким же образом доапгрейдживает модель, внося вот эти вот низкоранговые матрицы к существующим параметрам. Кстати, эти низкоранговые матрицы — их еще называют низкоранговыми адаптерами. И механизм <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">LoRA</code> был представлен в 2021 году, а в 2023 году представили его усовершенствованную, ну точнее не усовершенствованную, альтернативную версию.</p>
    <p class="whitespace-normal break-words">00:24:46 --- Называется <strong>QLoRA</strong>, либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Quantized Low-Rank Adaptation</code>. И, да, собственно, из названия все понятно. Мы сначала квантизируем наши матрицы, а потом уже делаем <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Low-Rank Adaptation</code>. И квантизация происходит до <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Q4</code>, то есть прям сильно точность весов уменьшается, соответственно, уменьшается качество модели при таком файн-тюнинге, но, как показывает практика, не сильно, и в каких-то задачах это обосновано.</p>
    <p class="whitespace-normal break-words">00:25:12 --- Потому что у нас, как бы, затраты на это обучение очень сильно уменьшаются по сравнению с другими методами. Это как если бы мы работали с каким-нибудь битмапом, с каким-нибудь изображением, и в процессе рисования добавили бы в это изображение новые объекты, а потом сохранили в JPEG. В принципе, для нашего глаза что битмап BMP, что JPEG — нормально, и то, и то одинаково. Но в JPEG сильно меньше данных, это сжатый формат. Вот примерно тем же самым занимается <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">QLoRA</code>.</p></div></div>
  <div><div class="grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0 !gap-3.5"><h1 class="text-2xl font-bold mt-1 text-text-100">Базовая терминология по AI: уровень "Pro"</h1>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:25:40 --- Inference</h2>
    <p class="whitespace-normal break-words">Следующий термин вы уже слышали даже сегодня в видео, я один раз его проронил, а может и несколько — это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">inference</code>, либо его еще называют <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">inference time compute</code>. По факту это процесс применения модели и ее параметров к новым входным данным.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:25:56 --- Как работает инференс</h2>
    <p class="whitespace-normal break-words">Когда мы запускаем нашу модель, загружаем веса в память, натравливаем на нее код, который обрабатывает эти веса, и начинаем подавать на вход какие-то данные, чтобы получить какой-то ответ — вот этот процесс называется инференсом. Если сравнивать с нашими повседневными разработческими задачами, можно сказать, что инференс — это рантайм для моделей. Просто рантайм. Вот мы запустили, вот оно работает. Инференситься еще иногда говорят. С точки зрения жизненного цикла модели, инференс является третьим этапом после обучения и развертывания модели.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:26:33 --- Где запускать инференс</h2>
    <p class="whitespace-normal break-words">Обучили, развернули, ну и, собственно, инференсим. Инференситься модели могут как на нашем локальном железе, если оно нам позволяет загружать веса в память, если у нас есть достаточное количество памяти, если у нас есть достаточное количество компьютера, чтобы обрабатывать эти веса. Если у нас нет этих мощностей, то мы можем арендовать мощности серверные. Сейчас куча разных инференсов.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:27:01 --- Инструменты для локального инференса</h2>
    <p class="whitespace-normal break-words">Если вы смотрите на open-source модельки, которые хотите локально запускать, то посмотрите на, например, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">LM Studio</code> — очень классный продукт. Либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">OLLAMA</code>, он чуть более низкоуровневый.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:27:30 --- Облачные инференсы и ресурсы</h2>
    <p class="whitespace-normal break-words">Если же вы смотрите на облачные инференсы, то можете посмотреть что-нибудь типа <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">OpenRouter</code>. И вообще куча всех этих решений есть, как для локальной работы, так и для облачной работы. Эти решения вы можете найти у нас в клубе, в чате инструменты по хэштегам <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">#inference</code>, либо по хэштегам <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">#model_providers</code>. По inference вы найдете скорее локальные варианты, которые можно установить локально работать. По Model Providers вы найдете кучу веб-провайдеров моделей, которые и запускают эти модели. Если они проводят нам доступ к этой модели, значит, они ее запускают, значит, они ее и инференсят. Более того, некоторые облачные провайдеры даже дают бесплатно компьютер, и дают немало бесплатного компьютера. Ссылочку внизу оставлю на один из сервисов Modelscope, который каждый месяц будет давать вам по 30 баксов на бесплатном аккаунте, а 30 баксов на самом деле хватает просто за глаза, чтобы использовать топовые опенсорсные модели в своих пет-проджектах.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:28:08 --- Классификация моделей</h2>
    <p class="whitespace-normal break-words">Теперь рассмотрим классификацию моделей по признакам их степени обученности и цели обучения. Тут про что я хочу сказать — про три типа моделей: Base модели, я вам уже сегодня несколько раз говорил этот термин, Instruct модели, тоже слышали, и Chat модели.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:28:24 --- Base модели</h2>
    <p class="whitespace-normal break-words">Собственно, с бейс-моделью все понятно. Это вот натренированная на широком спектре данных модель, которая прошла притрейн-процесс, вот этот вот процесс долгого обучения. После этого процесса обычно мы получаем бейс-модель, то есть базовую модель, из которой можно делать на самом деле много чего побочного. Много других моделей из нее можно делать. Часто такая модель не специализирована, то есть она максимально обобщена, она покрывает максимально широкий спектр задачи.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:28:52 --- Instruct модели</h2>
    <p class="whitespace-normal break-words">Часто такие модели дальше дотренировывают, дофайн-тюнивают, до чего-нибудь делают, чтобы они уже конкретно сильно лучше работали в определенных задачах. Инстракт-модель — это дотренированная версия бейс-модели, которая нацелена именно на то, чтобы качественно следовать инструкциям, качественно выполнять то, о чем ее просят. Сделать из бейс-моделей инстракт-модель можно с помощью обычного файн-тюнинга.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:29:16 --- Chat модели</h2>
    <p class="whitespace-normal break-words">И дальше у нас есть чат-модель — это дообученная, зафайн-тюниная версия инстракт-модели обычно, хотя она может быть получена на самом деле из бейс-модели, которая направлена, как вы поняли, на то, чтобы работать в чатах в виде ассистентов, в виде помощников. Потому что следовать инструкциям и чатиться в чате — это сильно два разных подхода к общению. Соответственно, на оба из этих подходов модели нужно дотренировать. Поэтому имеем базовую модель и от нее ответвление, например, на инстракт- и чат-модели.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:29:47 --- Почему это важно</h2>
    <p class="whitespace-normal break-words">Почему это важно? Потому что в названиях моделей вы будете часто видеть в конце еще приписочку <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">chat</code> либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">instruct</code> либо <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">base</code> — теперь вы будете знать, что эта приписочка означает. И кстати, то, что base модель нужно доучивать до чата либо инстракта, вообще не значит, что бейс модель сама по себе ни на что не годная. Часто, особенно крупные бейс модели, настолько качественного уровня получается, что с ними в принципе уже можно и чатиться, и при должном промпте инструкциям заставить их следовать.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:30:13 --- Практические рекомендации</h2>
    <p class="whitespace-normal break-words">В общем, все отразделение на инстракт и чат — это скорее больше семантика, хотя, конечно, какой-то процесс под этим есть, как минимум процесс fine-tuning небольшого. Что-то еще важно: если вы будете использовать open-source модели для того, чтобы подключать их в IDE-шку и чатиться с ними, то тут лучше всего, конечно, использовать чат-модели. Но если вы хотите использовать модель для автокомплита, то чат-модели тут не подойдут — они будут вам генерировать полную лухту. Выбирайте инстракт модели, ну, либо на худой конец бейс, но лучше инстракт.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:30:45 --- Merged модели</h2>
    <p class="whitespace-normal break-words">То есть для автокомплита — инстракт, для чатовой ДЕшки — чат модели. Еще вам возможно встретиться так называемые мердж модели, мержд модели, то есть смерженные модели, слитые, соединенные, объединенные модели.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:31:00 --- Как работает мердж</h2>
    <p class="whitespace-normal break-words">Можно объединять модели одна с другой, получать что-то среднее между ними для того, чтобы получить какой-то необходимый функционал из одной и из второй. Также вы можете совместить знания этих моделей, вы можете в целом улучшить эффективность их работы и получить функции из обоих этих моделей. И да, мы тут жертвуем, безусловно, какой-то частью из одной модели, какой-то частью из другой модели, но, тем не менее, мержд модели существуют, их часто делают.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:31:38 --- Требования к мерджу</h2>
    <p class="whitespace-normal break-words">Обычно мержд сливать модели можно, если у них достаточно сильная корреляция их параметров с базовой моделью, с которой они были сделаны. То есть архитектура строения параметров должна быть похожа между двумя моделями для того, чтобы их можно было соединить воедино, что в принципе логично. Как это работает под капотом не столь важно, просто знайте, что есть мердж модели и часто их делают для того, чтобы решать какую-то конкретную задачу лучше, чем модели, из которых они были составлены.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:32:06 --- Дистилляция модели</h2>
    <p class="whitespace-normal break-words">Дистилляция модели, либо дистиллированная модель — это тоже такой класс моделей, который характеризуется способом их получения. Дистилляция — это такой способ, когда маленькая модель получается путем обучения либо супервайзинга более старшей модели. То есть, по факту, более крутая модель, обычно из того же семейства, обучает более слабую модель. И таким способом получаются действительно очень крутые результаты.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:32:26 --- Пример с Llama</h2>
    <p class="whitespace-normal break-words">Например, дистиллят <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Llama 3.1</code> 405 миллиардов параметров, который дистиллирован в <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Llama 3.1</code> на 8 миллиардов параметров, то есть 405-миллиардная моделька обучила 8-миллиардную, работает лучше, чем <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Llama 3.1</code> 70 миллиардов параметров. Поэтому дистилляция — это важный процесс, важный механизм, который сейчас очень часто используется для того, чтобы модели с сильно меньшим количеством параметров работали значительно лучше, чем модели с большим количеством.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:32:57 --- Прунинг</h2>
    <p class="whitespace-normal break-words">Прунинг, либо прореживание, наряду с дистилляцией, с квантизацией, с мерджем — это ещё один из способов сделать модель более лёгкой, для того чтобы можно было её запускать на более слабом железе, при этом жертвуя кое-какими возможностями. Суть метода заключается в том, что выделяются веса либо кластера весов, которые не влияют либо влияют незначительно на качество финального ответа нашей модельки.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:33:23 --- Как работает прунинг</h2>
    <p class="whitespace-normal break-words">И эти нейрончики, эти веса просто убиваются, таким образом происходит прореживание. Сам процесс прореживания может включать в себя оценку важности весов и установку специального граничного значения, до которого, если веса после этой оценки не дотягивают, просто удаляются из нашей модели. Есть разные реализации, но в целом знайте, что прунинг — это обычное прореживание, удаление ненужных весов.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:33:50 --- Mixture of Experts</h2>
    <p class="whitespace-normal break-words"><code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Mixture of Experts</code> — наверняка вы это слышали, ну, либо еще услышите. Это еще сокращенно называется <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">MOE</code>, либо смесь экспертов. Это подход, при котором несколько специализированных моделей, их еще называют эксперты, обучаются на разных данных для выполнения разных задач. Для каждого входного примера специальный гейтовый механизм определяет, какой эксперт сможет дать лучший ответ, соответственно, занимается роутингом входных сообщений на разных экспертов.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:34:20 --- Аналогия с программированием</h2>
    <p class="whitespace-normal break-words">Это похоже на стратегию в программировании, когда для конкретной задачи выбирается конкретная реализация того-либо иного модуля, того-либо иного интерфейса. И на самом деле, много крупных моделей сейчас являются смесью экспертов. <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">GPT-4</code> являлся смесью экспертов.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:34:42 --- Mixtral как пример</h2>
    <p class="whitespace-normal break-words">Наверное, самым популярным Mixture of Expert является <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Mixtral</code>. Он так и называется, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Mixtral 8x7b</code>. И в названии, где вы видите что-то x что-то, обычно подразумевается, что эта модель является смесью экспертов. 8 — это количество экспертов, а x7b — это размерность этих экспертов, то есть то, на какое количество параметров эти эксперты опираются.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:35:09 --- Shared parameters</h2>
    <p class="whitespace-normal break-words">Тут надо отметить, что, например, <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">Mixtral 8x7b</code> не состоит из 54 миллиардов параметров, когда мы просто берем 8 и умножаем на 7. Эксперты опираются на некоторые слои в модели общие, те, которые не нужно им специализировать, и поэтому финальный размер модели, финальное количество параметров этой модели, оно меньше, чем вот это произведение количества экспертов на размерность каждого из экспертов, просто потому, что у них есть шареные параметры.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:35:42 --- SOTA</h2>
    <p class="whitespace-normal break-words">И последний на сегодня термин, закончим простеньким, хотя звучит оно нифига не простенько — <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">SOTA</code>. Вот вы наверняка слышали это слово SOTA, вы видели это сочетание букв, эту аббревиатуру, везде, где не попадя, пишут SOTA. Эта модель SOTA. Этот метод SOTA. Этот бенчмарк SOTA. SOTA. SOTA. SOTA. SOTA. Что за SOTA? Мед? Нет? Нет. Что за SOTA такая?</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:36:13 --- Расшифровка SOTA</h2>
    <p class="whitespace-normal break-words">SOTA — это <code class="bg-text-200/5 border border-0.5 border-border-300 text-danger-000 whitespace-pre-wrap rounded-[0.4rem] px-1 py-px text-[0.9rem]">State of the Art</code>. Что-то самое передовое. Это буквально все, что означает это SOTA, и когда вы видите очередную статью, даже научную, где написано SOTA подход к вниманию, то это значит просто, что вот эти ученые решили найти самый крутой на данный момент способ внимания, и больше ничего это не значит. Соответственно, выходит новая самая крутая модель, например, GPT-O1, она теперь является SOTA среди моделей. State of the Art. Никто не дает официальное звание SOTA моделям, просто люди для того, чтобы еще больше подчеркнуть уникальность своей модели, либо своего метода, своей технологии пишут, что это SOTA.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:36:33 --- Заключение</h2>
    <p class="whitespace-normal break-words">И эта SOTA заполонила весь мир и головы всех людей. Надеюсь, я вам сегодня голову не сильно заполонил какой-то сложной терминологией. Надеюсь, что более-менее было понятно, про что я вам сегодня рассказывал. Если что-то из услышанного вы не поняли, то можете переслушать это видео, можете потом пойти углить, можете посмотреть текстовую версию базового AI-словаря для разработчиков. Там все эти термины есть в более сжатой форме.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:37:03 --- Подведение итогов</h2>
    <p class="whitespace-normal break-words">Можно посмотреть каждый из них, зауглить каждый из них и узнать больше информации о терминологии. В целом я вас поздравляю, мы прошли с вами три основных части терминологии из AI для разработчиков.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:37:16 --- Что дальше</h2>
    <p class="whitespace-normal break-words">Это терминология уровня новичок, юзер и про, и ее вам будет абсолютно полностью достаточно для того, чтобы понимать, что происходит в клубе, что происходит в мире нейронок и программирования. Ну, а следующие следующие два видео будут посвящены уже чуть более глубокой терминологии для работы непосредственно с самими нейросетями и для того, чтобы понимать футурологию, так что следите за обновлениями. Я думаю, следующее видео выйдут достаточно скоро. Учите терминологию, применяйте те знания, которые получаете в клубе.</p>
    <h2 class="text-xl font-bold text-text-100 mt-1 -mb-0.5">00:37:53 --- Прощание</h2>
    <p class="whitespace-normal break-words">Ну и увидимся в следующих видео и в следующих клубных контентах. Всем пока, нейрофолкс!</p></div></div>
</template>